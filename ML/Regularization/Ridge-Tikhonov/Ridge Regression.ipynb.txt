{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "#np.set_printoptions(edgeitems=3,infstr='inf', linewidth=75, nanstr='nan', precision=8, suppress=False, threshold=1000, formatter=None)\n",
    "\n",
    "#numpy.eye(N, M=None, k=0, dtype=<class 'float'>, order='C')[source]\n",
    "#Return a 2-D array with ones on the diagonal and zeros elsewhere.\n",
    "#N : int           # Number of rows in the output.\n",
    "#M : int, optional #Number of columns in the output. If None, defaults to N.\n",
    "#k : int, optional #Index of the diagonal: 0 (the default) refers to the main diagonal, \n",
    "#                  #a positive value refers to an upper diagonal, and a negative value to a lower diagonal.\n",
    "print(np.eye(2, dtype=int))\n",
    "print(np.eye(3, k=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr2D = np.array([[11 ,12,13,11], [21, 22, 23, 24], [31,32,33,34]])\n",
    "print('Shape of arr2D: ', arr2D.shape)\n",
    "print('2D Numpy Array\\n',arr2D)\n",
    "# get number of rows in 2D numpy array\n",
    "print('numOfRows =', arr2D.shape[0])\n",
    "# get number of columns in 2D numpy array\n",
    "print('numOfColumns =', arr2D.shape[1])\n",
    "print('Total Number of elements in 2D Numpy array : ', arr2D.shape[0] * arr2D.shape[1])\n",
    "# Create a Numpy array from list of numbers\n",
    "arr = np.array([1, 2, 3, 4, 5, 6, 7, 8])\n",
    "print(arr)\n",
    "print('Shape of 1D numpy array : ', arr.shape)\n",
    "print('length of 1D numpy array : ', arr.shape[0])\n",
    "print(\"в Квадрате\",np.power(arr, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Ridge Regression: Scikit-learn vs. direct calculation does not match for alpha > 0](https://stackoverflow.com/questions/38562701/ridge-regression-scikit-learn-vs-direct-calculation-does-not-match-for-alpha)\n",
    "\n",
    "```python\n",
    "np.linalg.solve(A.T*A + alpha * I, A.T*b)\n",
    "```\n",
    "yields the same as\n",
    "```python\n",
    "np.linalg.inv(A.T*A + alpha * I)*A.T*b\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "# -*- coding: utf-8 -*-\n",
    "#https://gist.github.com/diogojc/1519756\n",
    "import numpy as np\n",
    "\n",
    "class RidgeRegressor(object):\n",
    "    \"\"\"\n",
    "    Linear Least Squares Regression with Tikhonov regularization. More simply called Ridge Regression.\n",
    "    We wish to fit our model so both the least squares residuals and L2 norm of the parameters are minimized.\n",
    "    argmin(Theta) = ||X*Theta - y||^2 + alpha * ||Theta||^2\n",
    "    A closed form solution is available.\n",
    "    Theta = (X'X + G'G)^-1 X'y\n",
    "    Where X contains the independent variables, y the dependent variable and G\n",
    "    is matrix alpha * I, where alpha is called the regularization parameter.\n",
    "    When alpha=0 the regression is equivalent to ordinary least squares.\n",
    "    \"\"\"\n",
    "    def fit(self, X, y, alpha=0):\n",
    "        \"\"\"\n",
    "        Fits our model to our training data.\n",
    "        Arguments\n",
    "        ----------\n",
    "        X: mxn matrix of m examples with n independent variables\n",
    "        y: dependent variable vector for m examples\n",
    "        alpha: regularization parameter. A value of 0 will model using the\n",
    "        ordinary least squares regression.\n",
    "        \"\"\"\n",
    "        X = np.hstack((np.ones((X.shape[0], 1)), X))\n",
    "        print(\"X.shape[0]\",X.shape[0],\"X.shape[1]\",X.shape[1]) #print(\"np.ones((X.shape[0], 1)) ==\\n\",np.ones((X.shape[0], 1)),end=''); print()\n",
    "        print(\"alpha=\",alpha)\n",
    "        G = alpha * np.eye(X.shape[1])\n",
    "        G[0, 0] = 0  # Don't regularize bias #print(G)\n",
    "        self.params = np.dot(np.linalg.inv( np.dot(X.T,X) +np.dot(G.T,G)),  \n",
    "                             np.dot(X.T,y))  \n",
    "        print(self.params)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predicts the dependent variable of new data using the model.\n",
    "        The assumption here is that the new data is iid to the training data.\n",
    "        Arguments\n",
    "        ----------\n",
    "        X: mxn matrix of m examples with n independent variables\n",
    "        alpha: regularization parameter. Default of 0.\n",
    "        Returns\n",
    "        ----------\n",
    "        Dependent variable vector for m examples\n",
    "        \"\"\"\n",
    "        X = np.hstack((np.ones((X.shape[0], 1)), X))\n",
    "        return np.dot(X, self.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, Math, Latex\n",
    "\n",
    "# Create synthetic data\n",
    "X = np.linspace(-4, 6, 31)\n",
    "b0=0; b1=0; b2=1; y = b0 +b1*X +b2*X*X\n",
    "\n",
    "# Plot synthetic data # plt.plot(X, y, label='$y = x^2$', linestyle='--',color='#00FF44')\n",
    "fig, ax = plt.subplots(nrows=1,ncols=2, figsize=(12,4))\n",
    "#----------1---------------\n",
    "ax[0].plot(X, y,                 label=r\"$y = x^2$\",               linestyle='--',      color='#00FF44')\n",
    "b0=4; b1=0; b2=1\n",
    "ax[0].plot(X, b0 +b1*X +b2*X*X,  label=r'$y ={0} + x^2$'.format(b0,b1,b2), linestyle='dashdot', color='red')\n",
    "b0=4; b1=4; b2=1\n",
    "ax[0].plot(X, b0 +b1*X +b2*X*X,  label=r'$y ={0} {1:+}x + x^2$'.format(b0,b1,b2), color='blue')\n",
    "b0=9; b1=-5; b2=2\n",
    "ax[0].plot(X, b0 +b1*X +b2*X*X,  label=r'$y ={0} {1:+}x {2:+}x^2$'.format(b0,b1,b2), color='magenta')\n",
    "ax[0].legend() #plt.legend()\n",
    "ax[0].set_ylim(-1,60);ax[0].set_xlim(-5, 6)\n",
    "ax[0].vlines([0],-1,100);ax[0].hlines([0],-5,56)\n",
    "#----------2---------------\n",
    "b0=9; b1=-5; b2=2\n",
    "ax[1].plot(X, b0 +b1*X +b2*X*X,  label=r'$y ={0} {1:+}x {2:+}x^2$'.format(b0,b1,b2), color='magenta')\n",
    "ax[1].legend() \n",
    "ax[1].set_ylim(-1,60);ax[1].set_xlim(-5, 6)\n",
    "ax[1].vlines([0],-1,60); ax[1].hlines([0],-5,15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input array \n",
    "in_arr1 = np.array([ 1, 2, 3] ) \n",
    "print (\"1st Input array : \\n\", in_arr1)  \n",
    "in_arr2 = np.array([ 4, 5, 6] ) \n",
    "print (\"2nd Input array : \\n\", in_arr2)  \n",
    "# Stacking the two arrays horizontally \n",
    "out_arr = np.hstack((in_arr1, in_arr2)) \n",
    "print (\"Output horizontally stacked array:\\n \", out_arr) \n",
    "#-----------------------------------\n",
    "# input array \n",
    "in_arr1 = np.array([[ 1, 2, 3], [ -1, -2, -3]] ) \n",
    "print (\"1st Input array : \\n\", in_arr1)  \n",
    "in_arr2 = np.array([[ 4, 5, 6], [ -4, -5, -6]] ) \n",
    "print (\"2nd Input array : \\n\", in_arr2)  \n",
    "# Stacking the two arrays horizontally \n",
    "out_arr = np.hstack((in_arr1, in_arr2)) \n",
    "print (\"Output stacked array :\\n \", out_arr) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(formatter={'float': '{:8.2f}'.format})\n",
    "#np.set_printoptions(edgeitems=3,infstr='inf', linewidth=75, nanstr='nan', precision=8, suppress=False, threshold=1000, formatter=None)\n",
    "print(X,'\\n'); print(y)\n",
    "yhat = y + 2.5 * np.random.normal(size = len(X))\n",
    "for ii in range(5): \n",
    "    #print(size//2 +ii)\n",
    "    yhat[len(X)//2 +ii] += 30\n",
    "print(yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,5))\n",
    "# Plot synthetic data\n",
    "plt.plot(X, y, 'g', label='$y = x^2$')\n",
    "plt.plot(X, yhat, 'rx', label='noisy samples')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, Math, Latex\n",
    "\n",
    "np.set_printoptions(formatter={'float': '{:8.3f}'.format})\n",
    "plt.plot(X, y, 'g', label='$y = x^2$')\n",
    "plt.plot(X, yhat, 'rx', label='noisy samples')\n",
    "\n",
    "# Create feature matrix\n",
    "tX = np.array([X]).T\n",
    "tX = np.hstack((tX, np.power(tX, 2), np.power(tX, 3)))\n",
    "\n",
    "# Plot regressors\n",
    "r = RidgeRegressor()\n",
    "alpha = 0.0\n",
    "r.fit(tX, y, alpha)\n",
    "plt.plot(X, r.predict(tX), 'b:', label=r'$ŷ (\\alpha={0:.1f})$'.format(alpha))\n",
    "\n",
    "alpha = 3.0\n",
    "r.fit(tX, y, alpha)\n",
    "plt.plot(X, r.predict(tX), 'yo', label=r'$ŷ (\\alpha={0:.1f})$'.format(alpha))\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "#########################################################\n",
    "fig, ax = plt.subplots(nrows=1,ncols=2, figsize=(14,4))\n",
    "#----------1---------------\n",
    "ax[0].plot(X, y, 'g', label='$y = x^2$')\n",
    "ax[0].plot(X, yhat, 'rx', label='noisy samples')\n",
    "#-------\n",
    "alpha = 0.0\n",
    "r.fit(tX, y, alpha)\n",
    "ax[0].plot(X, r.predict(tX), 'b:', label=r'$ŷ (\\alpha={0:.1f})$'.format(alpha))\n",
    "#-------\n",
    "alpha = 3.0\n",
    "r.fit(tX, y, alpha)\n",
    "ax[0].plot(X, r.predict(tX), 'yo', label=r'$ŷ (\\alpha={0:.1f})$'.format(alpha))\n",
    "ax[0].legend() #plt.legend()\n",
    "ax[0].set_ylim(-5,40);ax[0].set_xlim(-5, 7) #ax[0].vlines([0],-1,100);ax[0].hlines([0],-5,56)\n",
    "#----------2---------------\n",
    "ax[1].plot(X, y, 'g', label='$y = x^2$')\n",
    "alpha = 0.0\n",
    "r.fit(tX, y, alpha)\n",
    "ax[1].plot(X, r.predict(tX), 'b:', label=r'$ŷ (\\alpha={0:.1f})$'.format(alpha))\n",
    "\n",
    "b0, b1, b2, b3 = [round(sd,3) for sd in r.params]\n",
    "ax[1].plot(X, b0 +b1*X +b2*X*X +b3*X*X*X, 'yo', label=r'$y ={0} {1:+}x {2:+}x^2 {3:+}x^3$'.format(b0,b1,b2,b3), color='magenta')\n",
    "ax[1].legend() \n",
    "ax[1].set_ylim(-1,40);ax[1].set_xlim(-5, 7)\n",
    "#ax[1].vlines([0],-1,60); ax[1].hlines([0],-5,15)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, Math, Latex\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1,ncols=2, figsize=(14,4))\n",
    "#----------1---------------\n",
    "ax[0].plot(X, y, 'g', label='$y = x^2$')\n",
    "ax[0].plot(X, yhat, 'rx', label='noisy samples')\n",
    "\n",
    "alpha = 10.0\n",
    "r.fit(tX, y, alpha)\n",
    "ax[0].plot(X, r.predict(tX), 'c--', label=r'$ŷ (\\alpha=%.1f)$' % alpha)\n",
    "ax[0].legend() #plt.legend()\n",
    "ax[0].set_ylim(-1,40);ax[0].set_xlim(-5, 7) #ax[0].vlines([0],-1,100);ax[0].hlines([0],-5,56)\n",
    "#----------2---------------\n",
    "ax[1].plot(X, y, 'g', label='$y = x^2$')\n",
    "\n",
    "b0, b1, b2, b3 = [round(sd,3) for sd in r.params]\n",
    "ax[1].plot(X, b0 +b1*X +b2*X*X +b3*X*X*X,  label=r'$y ={0} {1:+}x {2:+}x^2 {3:+}x^3$'.format(b0,b1,b2,b3), color='magenta')\n",
    "ax[1].plot(X, r.predict(tX), 'c--', label=r'$ŷ (\\alpha=%.1f)$' % alpha)\n",
    "ax[1].legend() \n",
    "ax[1].set_ylim(-1,40);ax[1].set_xlim(-5, 7)\n",
    "#ax[1].vlines([0],-1,60); ax[1].hlines([0],-5,15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, Math, Latex\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1,ncols=2, figsize=(14,4))\n",
    "#----------1---------------\n",
    "ax[0].plot(X, y, 'g', label='$y = x^2$')\n",
    "ax[0].plot(X, yhat, 'rx', label='noisy samples')\n",
    "\n",
    "alpha = 20.0\n",
    "r.fit(tX, y, alpha)\n",
    "ax[0].plot(X, r.predict(tX), 'c--', label=r'$ŷ (\\alpha=%.1f)$' % alpha)\n",
    "ax[0].legend() #plt.legend()\n",
    "ax[0].set_ylim(-1,40);ax[0].set_xlim(-5, 7)\n",
    "#----------2---------------\n",
    "ax[1].plot(X, y, 'g', label='$y = x^2$')\n",
    "\n",
    "b0, b1, b2, b3 = [round(sd,3) for sd in r.params]\n",
    "ax[1].plot(X, b0 +b1*X +b2*X*X +b3*X*X*X,  label=r'$y ={0} {1:+}x {2:+}x^2 {3:+}x^3$'.format(b0,b1,b2,b3), color='magenta')\n",
    "ax[1].plot(X, r.predict(tX), 'c--', label=r'$ŷ (\\alpha=%.1f)$' % alpha)\n",
    "ax[1].legend() \n",
    "ax[1].set_ylim(-1,40);ax[1].set_xlim(-5, 7)\n",
    "#ax[1].vlines([0],-1,60); ax[1].hlines([0],-5,15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, Math, Latex\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1,ncols=2, figsize=(14,4))\n",
    "#----------1---------------\n",
    "ax[0].plot(X, y, 'g', label='$y = x^2$')\n",
    "ax[0].plot(X, yhat, 'rx', label='noisy samples')\n",
    "\n",
    "alpha = 40.0\n",
    "r.fit(tX, y, alpha)\n",
    "ax[0].plot(X, r.predict(tX), 'c--', label=r'$ŷ (\\alpha=%.1f)$' % alpha)\n",
    "ax[0].legend() #plt.legend()\n",
    "ax[0].set_ylim(-1,40);ax[0].set_xlim(-5, 7)\n",
    "#----------2---------------\n",
    "ax[1].plot(X, y, 'g', label='$y = x^2$')\n",
    "\n",
    "b0, b1, b2, b3 = [round(sd,3) for sd in r.params]\n",
    "ax[1].plot(X, b0 +b1*X +b2*X*X +b3*X*X*X,  label=r'$y ={0} {1:+}x {2:+}x^2 {3:+}x^3$'.format(b0,b1,b2,b3), color='magenta')\n",
    "ax[1].plot(X, r.predict(tX), 'c--', label=r'$ŷ (\\alpha=%.1f)$' % alpha)\n",
    "ax[1].legend() \n",
    "ax[1].set_ylim(-1,40);ax[1].set_xlim(-5, 7)\n",
    "#ax[1].vlines([0],-1,60); ax[1].hlines([0],-5,15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [RIDGE REGRESSION AND ITS IMPLEMENTATION WITH PYTHON](https://mlforanalytics.com/2018/05/22/ridge-regression-and-its-implementation-with-python/)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "np.set_printoptions(formatter={'float': '{:5.2f}'.format})\n",
    "import numpy as np              #importing the numpy package with alias np\n",
    "import matplotlib.pyplot as plt #importing the matplotlib.pyplot as plt\n",
    "\n",
    "B0=1\n",
    "B1=0.5\n",
    "No_of_observations = 51                         #Setting number of observation = 50\n",
    "X_input = np.linspace(0,10,No_of_observations)  #Generating 50 equally-spaced data points between 0 to 10.\n",
    "Y_output =B0+B1*X_input + np.random.randn(No_of_observations)   #setting Y_outputi = 0.5X_inputi + some random noise\n",
    "print(X_input)\n",
    "print(Y_output)\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(16,6))\n",
    "#----------1---------------\n",
    "ax[0].plot(X_input,B0+B1*X_input , '--', label=r\"$y ={0:} {1:+}*x$\".format(B0,B1))\n",
    "ax[0].plot(X_input, Y_output, 'bo', label=r\"$y = \\frac{1}{2}*x$ +Random\",   color='#0000FF')\n",
    "ax[0].title.set_text('Relationship between Y and X[:, 1]')\n",
    "ax[0].legend()\n",
    "ax[0].set_ylim(-5,40);ax[0].set_xlim(-1, 11)\n",
    "ax[0].vlines([0],-10,100);ax[0].hlines([0],-5,56)\n",
    "#----------2---------------\n",
    "ax[1].plot(X_input,B0+B1*X_input , '--', label=r\"$y ={0:} {1:+}*x$\".format(B0,B1))\n",
    "Y_output[-1]+=30   #setting last element of Y_output as Y_output + 30\n",
    "Y_output[-2]+=30   #setting second last element of Y_output as Y_output + 30\n",
    "Y_output[-3]+=30\n",
    "ax[1].plot(X_input, Y_output, 'bo', label=r\"$y ={0:} {1:+}*x$ +Random\".format(B0,B1),   color='#00FF44')\n",
    "ax[1].title.set_text('Relationship between Y and X[:, 1]')\n",
    "ax[1].set_ylim(-5,40);ax[1].set_xlim(-1, 11)\n",
    "ax[1].vlines([0],-10,100);ax[1].hlines([0],-5,56)\n",
    "ax[1].legend()\n",
    "#----------3---------------\n",
    "ax[2].plot(X_input,B0+B1*X_input , '--', label=r\"$y ={0:} {1:+}*x$\".format(B0,B1))\n",
    "ax[2].plot(X_input, Y_output, 'bo', label=r\"$y ={0:} {1:+}*x$ +Random\".format(B0,B1),   color='#00FF44')\n",
    "ax[2].title.set_text('Graph of maximum likelihood method(Red line: predictions)')\n",
    "ax[2].set_ylim(-5,40);ax[2].set_xlim(-1, 11)\n",
    "ax[2].vlines([0],-10,100);ax[2].hlines([0],-5,56)\n",
    "##########Средняя Квадратичная #################################\n",
    "X_input2 = np.vstack([np.ones(No_of_observations), X_input]).T       #appending bias data points colummn to X\n",
    "for i in range(9): print(X_input2[i], end='')\n",
    "#finding weights for maximum likelihood estimation    \n",
    "w_maxLikelihood = np.linalg.solve(np.dot(X_input2.T, X_input2), np.dot(X_input2.T, Y_output))     \n",
    "Y_maxLikelihood = np.dot(X_input2, w_maxLikelihood)\n",
    "\n",
    "ll=\"$y={0:3.2f} {1:+3.2f}x$\".format(w_maxLikelihood[0], w_maxLikelihood[1])\n",
    "ax[2].plot(X_input, Y_maxLikelihood, '--', label=ll, color='red')\n",
    "ax[2].plot(X_input[::4],[w_maxLikelihood[0] + w_maxLikelihood[1]*xx for xx in X_input[::4]], 'bo', label=ll, color='y'  )\n",
    "ax[2].legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(X_input,B0+B1*X_input , '--', label=r\"$y ={0:} {1:+}*x$\".format(B0,B1))\n",
    "plt.plot(X_input, Y_output, 'bo', label=r\"$y ={0:} {1:+}*x$ +Random\".format(B0,B1), color='#4169E1') #color='royalblue')\n",
    "ll=\"$y={0:3.2f} {1:+3.2f}x$\".format(w_maxLikelihood[0], w_maxLikelihood[1])\n",
    "plt.plot(X_input2[:,1],Y_maxLikelihood, '--', label=ll, color='red')\n",
    "plt.plot(X_input[::4],[w_maxLikelihood[0] + w_maxLikelihood[1]*xx for xx in X_input[::4]], 'bo', color='y', label=ll)\n",
    "\n",
    "L2_coeff = 10    #setting L2 regularization parameter to 1000\n",
    "#Finding weights for MAP estimation\n",
    "w_maxAPosterior = np.linalg.solve(np.dot(X_input2.T, X_input2)+L2_coeff*np.eye(2), np.dot(X_input2.T, Y_output))  \n",
    "Y_maxAPosterior = np.dot(X_input2, w_maxAPosterior) #Finding predicted Y corresponding to w_maxAPosterior\n",
    "plt.plot(X_input[::4],Y_maxAPosterior[::4], '--o', color='m', label=\"Ridge[{0:}]\".format(L2_coeff))\n",
    "plt.title('Graph of MAP v/s ML method')\n",
    "plt.legend()\n",
    "plt.xlabel('X[:, 1]')\n",
    "plt.ylabel('Y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from matplotlib.colors import ListedColormap\n",
    "import matplotlib.pylab as pl\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "\n",
    "plt.plot(X_input,B0 +B1*X_input , '--', label=r\"$y ={0:} {1:+}*x$\".format(B0,B1))\n",
    "plt.plot(X_input, Y_output, 'bo', label=r\"$y ={0:} {1:+}*x$ +Random\".format(B0,B1), color='#4169E1') #color='royalblue')\n",
    "ll=\"$y={0:3.2f} {1:+3.2f}x$\".format(w_maxLikelihood[0], w_maxLikelihood[1])\n",
    "plt.plot(X_input2[:,1],Y_maxLikelihood, '--', label=ll, color='red')\n",
    "plt.plot(X_input[::4],[w_maxLikelihood[0] + w_maxLikelihood[1]*xx for xx in X_input[::4]], 'bo', color='y', label=ll)\n",
    "\n",
    "steps = 12; rising =100\n",
    "Y_maxAPosterior = []\n",
    "w_maxAPosterior = []\n",
    "colors = pl.cm.jet(np.linspace(0,2,steps))\n",
    "\n",
    "for i in range(0,steps):\n",
    "    L2_coeff = (i+1)*rising    #setting L2 regularization parameter to 1000\n",
    "    w_maxAPosterior.append(np.linalg.solve(np.dot(X_input2.T, X_input2)+L2_coeff*np.eye(2), np.dot(X_input2.T, Y_output)))\n",
    "    Y_maxAPosterior.append(np.dot(X_input2, w_maxAPosterior[i])) #Finding predicted Y corresponding to w_maxAPosterior\n",
    "    plt.plot(X_input[::4],Y_maxAPosterior[i][::4], '--', color=colors[i], label=\"Ridge[L2 == {0}]\".format(L2_coeff))\n",
    "    plt.title('Ridge [from {0:} to {1:}] '.format(1*rising,rising*steps))\n",
    "    plt.legend()\n",
    "\n",
    "plt.xlabel('X[:, 1]')\n",
    "plt.ylabel('Y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Норма (математика)](https://ru.wikipedia.org/wiki/%D0%9D%D0%BE%D1%80%D0%BC%D0%B0_(%D0%BC%D0%B0%D1%82%D0%B5%D0%BC%D0%B0%D1%82%D0%B8%D0%BA%D0%B0))\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "<td style=\"width:40%;vertical-align:top; text-align:left; font-size:medium;\">\n",
    "<li>$\\|x\\|_1 = \\sum_{i} |x_{i}|$, что также имеет название $\\textit{метрика L1}$, норма $\\ell_1$ или [Расстояние_городских_кварталов|манхэттенское расстояние](). Для вектора представляет собой сумму модулей всех его элементов. $\\left\\| x \\right\\| = \\left| x \\right|$\n",
    "<li>$\\|x\\|_2 = \\sqrt{\\sum_{i} |x_{i}|^2}$, что также имеет название $\\textit{метрика L2}$, норма $\\ell_2$ \n",
    "    или <a href=\"https://en.wikipedia.org/wiki/Norm_(mathematics)#Euclidean_norm\">евклидова норма</a>. Является геометрическим расстоянием между двумя точками в многомерном пространстве, вычисляемым по теореме Пифагора. $\\left\\| \\boldsymbol{x} \\right\\|_2 := \\sqrt{x_1^2 + \\cdots + x_n^2}$\n",
    "<li>$\\|x\\|_p = \\sqrt[p]{\\sum\\limits_{n=1}^{\\infty}|x_n|^p}$ Пространства $\\ell^p$-norm\n",
    "$\\|x\\|_p = \\sqrt[p]{|x_1|^p + |x_2|^p + \\dotsb + |x_n|^p}$\n",
    "</td>\n",
    "<td>\n",
    "<img width=300 src=\"images/440px-Vector-p-Norms_qtl1.svg.png\" />\n",
    "</td>\n",
    "<td>\n",
    "<img width=300 src=\"images/Boules-unite-en-normes-p--avec-quadrillage-et-equation.png\" />\n",
    "</td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Собственный вектор](https://ru.wikipedia.org/wiki/%D0%A1%D0%BE%D0%B1%D1%81%D1%82%D0%B2%D0%B5%D0%BD%D0%BD%D1%8B%D0%B9_%D0%B2%D0%B5%D0%BA%D1%82%D0%BE%D1%80)\n",
    "\n",
    "**Со́бственный ве́ктор** — понятие в [линейной алгебре](https://ru.wikipedia.org/wiki/%D0%9B%D0%B8%D0%BD%D0%B5%D0%B9%D0%BD%D0%B0%D1%8F_%D0%B0%D0%BB%D0%B3%D0%B5%D0%B1%D1%80%D0%B0), определяемое для произвольного [линейного оператора](https://ru.wikipedia.org/wiki/%D0%9B%D0%B8%D0%BD%D0%B5%D0%B9%D0%BD%D0%BE%D0%B5_%D0%BE%D1%82%D0%BE%D0%B1%D1%80%D0%B0%D0%B6%D0%B5%D0%BD%D0%B8%D0%B5) как ненулевой [вектор](https://ru.wikipedia.org/wiki/%D0%92%D0%B5%D0%BA%D1%82%D0%BE%D1%80_(%D0%BC%D0%B0%D1%82%D0%B5%D0%BC%D0%B0%D1%82%D0%B8%D0%BA%D0%B0)), применение к которому оператора даёт [коллинеарный](https://ru.wikipedia.org/wiki/%D0%9A%D0%BE%D0%BB%D0%BB%D0%B8%D0%BD%D0%B5%D0%B0%D1%80%D0%BD%D0%BE%D1%81%D1%82%D1%8C) вектор — тот же вектор, умноженный на некоторое скалярное значение. Скаляр, на который умножается собственный вектор под действием оператора, называется **собственным числом** (или **собственным значением**) линейного оператора, соответствующим данному собственному вектору. Одним из представлений линейного оператора является [квадратная матрица](https://ru.wikipedia.org/wiki/%D0%9A%D0%B2%D0%B0%D0%B4%D1%80%D0%B0%D1%82%D0%BD%D0%B0%D1%8F_%D0%BC%D0%B0%D1%82%D1%80%D0%B8%D1%86%D0%B0), поэтому собственные векторы и собственные значения часто определяются в контексте использования таких матриц.\n",
    "\n",
    "Понятия собственного вектора и собственного числа являются одними из ключевых в линейной алгебре, на их основе строится множество конструкций. Это связано с тем, что многие соотношения, связанные с линейными операторами, существенно упрощаются в системе координат, построенной на [базисе](https://ru.wikipedia.org/wiki/%D0%91%D0%B0%D0%B7%D0%B8%D1%81) из собственных векторов оператора. Множество собственных значений линейного оператора ([спектр оператора](https://ru.wikipedia.org/wiki/%D0%A1%D0%BF%D0%B5%D0%BA%D1%82%D1%80_%D0%BE%D0%BF%D0%B5%D1%80%D0%B0%D1%82%D0%BE%D1%80%D0%B0)) характеризует важные свойства оператора без привязки к какой-либо конкретной системе координат.\n",
    "\n",
    "Понятие линейного векторного пространства не ограничивается «чисто геометрическими» векторами и обобщается на разнообразные множества объектов, таких как пространства функций (в которых действуют линейные дифференциальные и интегральные операторы). Для такого рода пространств и операторов говорят о собственных функциях операторов.\n",
    "\n",
    "Множество всех собственных векторов линейного оператора, соответствующих данному собственному числу, дополненное [нулевым вектором](https://ru.wikipedia.org/wiki/%D0%9D%D1%83%D0%BB%D0%B5%D0%B2%D0%BE%D0%B9_%D0%B2%D0%B5%D0%BA%D1%82%D0%BE%D1%80), называется собственным подпространством этого оператора.\n",
    "\n",
    "$$\\textbf{A}\\,\\vec x  = \\vec y$$\n",
    "\n",
    "$$\\begin{equation}\\textbf{A}\\,\\vec x  = \\lambda \\vec x \\label{eq:eq1} \\end{equation}$$\n",
    "Предположим что существует такой вектор не равный $\\vec x  \\ne \\vec 0 $ тогда:\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\textbf{A}\\vec x  - \\lambda \\vec x  & = \\vec 0\\\\\n",
    "\\textbf{A}\\vec x  - \\lambda {\\textbf{I}_n}\\vec x  & = \\vec 0\\\\\n",
    "\\left( {\\textbf{A} - \\lambda {\\textbf{I}_n}} \\right)\\vec x & = \\vec 0\n",
    "\\end{align*}$$\n",
    "\n",
    "Если имеется собственный вектор не равный 0 то тогда:\n",
    "\n",
    "$$|\\textbf{A} - \\lambda \\textbf{I}| = 0$$\n",
    "\n",
    "Пусть имеется квадратная матрица $A$ разменостью $n \\times n$:\n",
    "$$\\begin{bmatrix}\n",
    "    a_{11} & a_{12} & a_{13} & \\dots  & a_{1n} \\\\\n",
    "    a_{21} & a_{22} & a_{23} & \\dots  & a_{2n} \\\\\n",
    "    \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    a_{d1} & a_{d2} & a_{d3} & \\dots  & a_{dn}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "**собственное значение** со связанным **собственным вектором**:\n",
    "$$\\begin{bmatrix}\n",
    "    a_{11} & a_{12} & a_{13} & \\dots  & a_{1n} \\\\\n",
    "    a_{21} & a_{22} & a_{23} & \\dots  & a_{2n} \\\\\n",
    "    \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    a_{n1} & a_{n2} & a_{n3} & \\dots  & a_{nn}\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "    x_1\\\\\n",
    "    x_2\\\\\n",
    "    \\vdots\\\\\n",
    "    x_n\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\lambda\n",
    "\\begin{bmatrix}\n",
    "    x_1\\\\\n",
    "    x_2\\\\\n",
    "    \\vdots\\\\\n",
    "    x_n\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Тогда эту систему можно преобразовать к:\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "    a_{11} -\\lambda & a_{12} & a_{13} & \\dots  & a_{1n} \\\\\n",
    "    a_{21} & a_{22} -\\lambda & a_{23} & \\dots  & a_{2n} \\\\\n",
    "    \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    a_{n1} & a_{n2} & a_{n3} & \\dots  & a_{nn} -\\lambda\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "    x_1\\\\\n",
    "    x_2\\\\\n",
    "    \\vdots\\\\\n",
    "    x_n\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "    0\\\\\n",
    "    0\\\\\n",
    "    \\vdots\\\\\n",
    "    0\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "В более компактной форме может быть записана как:\n",
    "\n",
    "$$\n",
    "(\\textbf{A} -\\lambda \\cdot \\textbf{I})\\textbf{X} = 0\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# [Ridge regression and L2 regularization - Introduction](https://xavierbourretsicotte.github.io/intro_ridge.html)\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "<td style=\"text-align: left;\">\n",
    "<b>Ordinary Least Squares</b>\n",
    "$$L_1(w) = \\sum_{i=1}^m (y_i - \\mathbf{x_i}^T w)^2 = (y - X w)^T (y - X w)$$\n",
    "\n",
    "$\\hat w_{ridge} = argmin_{w \\in \\mathbb{R}^n} \\sum_{i=1}^m (y_i - \\mathbf{x_i}^T w)^2 ==$\n",
    "\n",
    "Берём производную и приравниваем к 0, в точке перегиба самый минимум:\n",
    "$$\\boxed{\\frac{\\partial L_1(w)}{\\partial w} = \\left[(b+wx-y)^2\\right]\\big|_w^\\prime = 0}$$\n",
    "\n",
    "$$2(b+wx - y)\\cdot x = 0$$\n",
    "$$(b+wx - y)\\cdot x = bx+ w \\cdot x^2 - y \\cdot x = 0 $$\n",
    "$$w \\cdot x^2 = y \\cdot x - bx$$\n",
    "$$w \\cdot x^2 = x(y-b)$$\n",
    "$$w = \\frac{x(y -b)}{(x^2)} = (x^2)^{-1} \\cdot x(y-b) \\iff \\text{убрали b} $$\n",
    "$$\\textbf{w} = \\boxed {\\mathbf{(X^TX)^{-1}X^Ty}}$$\n",
    "\n",
    "<b>Ridge regression</b>\n",
    "Добавляется маленькая добавка к матрице по диагонали чтобы eigen-values(собственное число) и матрица стала в более стабильном состоянии:\n",
    "$$\\hat {\\textbf{w}}_{ridge} = argmin_{\\textbf{w} \\in \\mathbb{R}^n} \\sum_{i=1}^m (y_i - \\mathbf{x_i}^T \\textbf{w})^2 + \\lambda \\sum_{j=1}^n \\textbf{w}^2$$\n",
    "\n",
    "**Quadratic cost function** изменим чуть-чуть значение $\\lambda$ - возведём в квадрат для облегчения вычислений:\n",
    "$$\\boxed{L_2(\\textbf{w}) = \\sum_{i=1}^m (y_i - \\mathbf{x_i}^T \\textbf{w})^2 + \\lambda^2 \\sum_{j=1}^n \\textbf{w}^2} $$\n",
    "$$\n",
    "\\begin{aligned}\n",
    "& = \\mathbf{(y - X \\textbf{w})^T(y - X \\textbf{w}) + \\lambda^2  \\textbf{w}^T  \\textbf{w}}\\\\\n",
    "& = \\mathbf{(y - X \\textbf{w})^T(y - X \\textbf{w}) + \\lambda^2 \\textbf{w}^T \\textbf{w}}\\\\\n",
    "& = \\mathbf{(y^T - \\textbf{w}^T X^T) (y - X \\textbf{w}) + \\lambda^2  \\textbf{w}^T  \\textbf{w}}\\\\\n",
    "& = \\mathbf{ y^Ty - y^T X\\textbf{w}-\\textbf{w}^T X^T y + \\textbf{w}^T X^T X\\textbf{w} +  \\lambda^2 \\textbf{w}^T \\textbf{w} }\\\\\n",
    "& = \\mathbf{ y^Ty  - 2 \\textbf{w}^TX^Ty + \\textbf{w}^T X^T X\\textbf{w} +  \\lambda^2 \\textbf{w}^T \\textbf{w}  }\\\\\n",
    "    \\frac{ \\partial L_2(w)}{\\partial w} &= \\mathbf{  - 2 X^Ty + 2 \\textbf{w} X^TX + 2 \\lambda^2 \\textbf{w}} = 0\\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "$$\\mathbf{  - X^T y + \\textbf{w} X^TX + \\lambda^2 \\textbf{w}} = 0 $$\n",
    "$$\\mathbf{\\textbf{w} X^TX + \\lambda^2 \\textbf{I} \\textbf{w}} = \\mathbf{X^T y} $$\n",
    "$$\\mathbf{\\textbf{w} (X^TX + \\lambda^2 \\textbf{I})} = \\mathbf{X^T y} $$\n",
    "$$\\boxed{\\textbf{w} = \\mathbf{ (X^TX + \\lambda^2I)^{-1} X^Ty}}$$\n",
    "\n",
    "\n",
    "</td>\n",
    "<td style=\"vertical-align: text-top;\">\n",
    "$$L_2 = (b+wx-y)^2+\\lambda w^2$$\n",
    "\n",
    "$$minimaze \\left\\|b+Xw-y\\right\\|_2^2 + \\left\\|\\Gamma x\\right\\|^2 ==$$\n",
    "\n",
    "$\\Gamma = \\alpha I$  и здесь $\\alpha = \\sqrt{\\lambda} \\frac{y}{x} = \\sqrt{\\lambda} \\cdot w$\n",
    "\n",
    "$$==\\sum_{i=1}^n (\\textbf{x}_i^T \\textbf{w} - y_i)^2 + \\lambda \\|\\textbf{w}\\|^2 = \\sum_{i=1}^n (\\textbf{x}_i^T \\textbf{w} - y_i)^2 + \\lambda \\textbf{w}^T\\textbf{w}$$\n",
    "\n",
    "$\\|\\textbf{w}\\|^2 = \\sqrt{w_1^2+w_2^2+\\cdots+w_n^2} =\\textbf{w}^T\\textbf{w}$\n",
    "\n",
    "Берём производную и приравниваем к 0, в точке перегиба самый минимум:\n",
    "$$\\boxed{\\frac{\\partial L_2(w)}{\\partial w} = \\left[(b+wx-y)^2+\\lambda w^2\\right]\\big|_w^\\prime = 0}$$\n",
    "\n",
    "$$2(b+wx - y)\\cdot x + 2\\lambda \\cdot w = 0$$\n",
    "$$(b+wx - y)\\cdot x + \\lambda \\cdot w =bx + w \\cdot x^2 - y \\cdot x + \\lambda \\cdot w = 0 $$\n",
    "$$w \\cdot x^2 + \\lambda \\cdot w  = y \\cdot x - bx$$\n",
    "$$w(x^2 + \\lambda)  = x (y -b)$$\n",
    "$$w = \\frac{x(y -b)}{(x^2 + \\lambda)} = (x^2 + \\lambda)^{-1} \\cdot x(y - b) \\iff \\text{убрали b}$$\n",
    "\n",
    "$$\\boxed{\\textbf{w} = (\\textbf{X}^T\\textbf{X} + \\lambda \\textbf{I})^{-1}\\textbf{X}^T\\textbf{y} }$$ \n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "[Factor selection and pivoted QR](http://www.cs.cornell.edu/courses/cs6210/2019fa/lec/2019-10-16.pdf)\n",
    "$minimize \\| Ax-y\\|^2 +\\lambda^2\\| x\\|^2 \\to$ тоже самое что и $\\to minimize \\left\\|\n",
    "\\begin{bmatrix}\n",
    "A\\\\\n",
    "\\lambda I\\\\\n",
    "\\end{bmatrix} \n",
    "x \n",
    "-\n",
    "\\begin{bmatrix}\n",
    "y\\\\\n",
    "0\\\\\n",
    "\\end{bmatrix} \n",
    "\\right\\|^2$:\n",
    "\n",
    "[Calculation of the squared Euclidean norm](https://math.stackexchange.com/questions/1865476/calculation-of-the-squared-euclidean-norm)\n",
    "$$\n",
    "\\left\\|\n",
    "\\begin{bmatrix}\n",
    "A\\\\\n",
    "\\lambda I\\\\\n",
    "\\end{bmatrix} \n",
    "x \n",
    "-\n",
    "\\begin{bmatrix}\n",
    "y\\\\\n",
    "0\\\\\n",
    "\\end{bmatrix} \n",
    "\\right\\|^2 \n",
    "=\n",
    "\\left\\|\n",
    "\\begin{bmatrix}\n",
    "A x\\\\\n",
    "\\lambda x\\\\\n",
    "\\end{bmatrix} \n",
    "-\n",
    "\\begin{bmatrix}\n",
    "y\\\\\n",
    "0\\\\\n",
    "\\end{bmatrix} \n",
    "\\right\\|^2\n",
    "=\n",
    "\\left(\n",
    "\\begin{bmatrix}\n",
    "A x\\\\\n",
    "\\lambda x\\\\\n",
    "\\end{bmatrix}\n",
    "-\n",
    "\\begin{bmatrix}\n",
    "y\\\\\n",
    "0\\\\\n",
    "\\end{bmatrix} \n",
    "\\right)^T\n",
    "\\cdot\n",
    "\\left(\n",
    "\\begin{bmatrix}\n",
    "A x\\\\\n",
    "\\lambda x\\\\\n",
    "\\end{bmatrix} \n",
    "-\n",
    "\\begin{bmatrix}\n",
    "y\\\\\n",
    "0\\\\\n",
    "\\end{bmatrix}\n",
    "\\right)\n",
    "==\n",
    "$$\n",
    "\n",
    "$$\n",
    "==\n",
    "\\left\\|\n",
    "\\begin{bmatrix}\n",
    "A x\\\\\n",
    "\\lambda x\\\\\n",
    "\\end{bmatrix} \n",
    "\\right\\|^2\n",
    "+\n",
    "\\left\\|\n",
    "\\begin{bmatrix}\n",
    "y\\\\\n",
    "0\\\\\n",
    "\\end{bmatrix} \n",
    "\\right\\|^2\n",
    "-\n",
    "\\left(\n",
    "\\begin{bmatrix}\n",
    "A x\\\\\n",
    "\\lambda x\\\\\n",
    "\\end{bmatrix} \n",
    "\\right)^T\n",
    "\\begin{bmatrix}\n",
    "y\\\\\n",
    "0\\\\\n",
    "\\end{bmatrix} \n",
    "-\n",
    "\\begin{bmatrix}\n",
    "y\\\\\n",
    "0\\\\\n",
    "\\end{bmatrix}^T\n",
    "\\left(\n",
    "\\begin{bmatrix}\n",
    "A x\\\\\n",
    "\\lambda x\\\\\n",
    "\\end{bmatrix} \n",
    "\\right)\n",
    "\\Longrightarrow\n",
    "$$\n",
    "\n",
    "$$\n",
    "u^T\\cdot v == v^T \\cdot u \\to\n",
    "\\left(\n",
    "\\begin{bmatrix}\n",
    "A\\\\\n",
    "\\lambda I\\\\\n",
    "\\end{bmatrix} \n",
    "x\\right)^T\n",
    "\\begin{bmatrix}\n",
    "y\\\\\n",
    "0\\\\\n",
    "\\end{bmatrix} \n",
    " ==\n",
    "\\begin{bmatrix}\n",
    "y\\\\\n",
    "0\\\\\n",
    "\\end{bmatrix}^T\n",
    "\\left(\n",
    "\\begin{bmatrix}\n",
    "A\\\\\n",
    "\\lambda I\\\\\n",
    "\\end{bmatrix} \n",
    "x\\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\Longrightarrow\n",
    "\\left\\|\n",
    "\\begin{bmatrix}\n",
    "Ax\\\\\n",
    "\\lambda x\\\\\n",
    "\\end{bmatrix} \n",
    "\\right\\|^2\n",
    "+\n",
    "\\left\\|\n",
    "\\begin{bmatrix}\n",
    "y\\\\\n",
    "0\\\\\n",
    "\\end{bmatrix}\n",
    "\\right\\|^2\n",
    "-2 \\cdot\n",
    "\\left(\n",
    "\\begin{bmatrix}\n",
    "Ax\\\\\n",
    "\\lambda x\\\\\n",
    "\\end{bmatrix} \n",
    "\\right)^T\n",
    "\\begin{bmatrix}\n",
    "y\\\\\n",
    "0\\\\\n",
    "\\end{bmatrix} \n",
    "==\n",
    "\\left\\|\n",
    "\\begin{bmatrix}\n",
    "Ax\\\\\n",
    "\\lambda x\\\\\n",
    "\\end{bmatrix} \n",
    "\\right\\|^2\n",
    "+\n",
    "y^2\n",
    "-\n",
    "2 \\cdot\n",
    "\\left(\n",
    "\\begin{bmatrix}\n",
    "Ax\\\\\n",
    "\\lambda x\\\\\n",
    "\\end{bmatrix} \n",
    "\\right)^T\n",
    "\\begin{bmatrix}\n",
    "y\\\\\n",
    "0\\\\\n",
    "\\end{bmatrix} \n",
    "==\n",
    "$$\n",
    "\n",
    "$$\n",
    "==\n",
    "\\left\\|\n",
    "\\begin{bmatrix}\n",
    "Ax\\\\\n",
    "\\lambda x\\\\\n",
    "\\end{bmatrix} \n",
    "\\right\\|^2\n",
    "+\n",
    "y^2\n",
    "-\n",
    "2 \\cdot\n",
    "\\left(\n",
    "\\begin{bmatrix}\n",
    "Ax\\\\\n",
    "\\lambda x\\\\\n",
    "\\end{bmatrix} \n",
    "\\right)^T\n",
    "\\begin{bmatrix}\n",
    "y\\\\\n",
    "0\\\\\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\left(\n",
    "\\begin{bmatrix}\n",
    "Ax\\\\\n",
    "\\lambda x\\\\\n",
    "\\end{bmatrix}\n",
    "\\right)^T\n",
    "\\left(\n",
    "\\begin{bmatrix}\n",
    "Ax\\\\\n",
    "\\lambda x\\\\\n",
    "\\end{bmatrix} \n",
    "\\right)\n",
    "+\n",
    "y^2\n",
    "-\n",
    "2 \\cdot Axy\n",
    "==\n",
    "$$\n",
    "\n",
    "$$\n",
    "== (Ax)^2 + (\\lambda x)^2+ y^2 - 2Axy = (Ax)^2 -2(Ax)y +y^2 +(\\lambda x)^2 \\Longrightarrow$$\n",
    "$$\\boxed{\\| Ax-y\\|^2 +\\lambda^2\\| x\\|^2}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Regularized Cost Function** \n",
    "\n",
    "$h_\\theta(x^{(i)}) = \\theta_0 x_0^{(i)} + \\theta_1 x_1^{(i)} + ... + \\theta_n x_n^{(i)}$\n",
    "\n",
    "$$\\begin{aligned}\n",
    "J(\\theta) &= \\frac{1}{2m}\\sum_{i=1}^m(h_\\theta(x^{(i)}) - y^{(i)})^2 + \\frac{\\lambda}{2m}\\sum_{j=1}^{n}\\theta_{j}^{2}\\\\\n",
    "J(\\theta) &= \\frac{1}{2m}(X\\theta - y)^T(X\\theta - y) + \\frac{\\lambda}{2m} \\theta^T \\theta \\mbox{(vectorized version)}\n",
    "\\end{aligned}$$\n",
    "\n",
    "[Why divide by ${2m}$ ?](https://math.stackexchange.com/questions/884887/why-divide-by-2m)\n",
    "* The $\\frac{1}{m}$ is to \"average\" the squared error over the number of components so that the number of components doesn't affect the function.\n",
    "\n",
    "[Why do we have to divide by 2 ?](https://datascience.stackexchange.com/questions/52157/why-do-we-have-to-divide-by-2-in-the-ml-squared-error-cost-function)\n",
    "* The $\\frac{1}{2}$ is because when you take the derivative of the cost function, that is used in updating the parameters during gradient descent, that $2$ in the power get cancelled with the $\\frac{1}{2}$ multiplier, thus the derivation is cleaner. These techniques are or somewhat similar are widely used in math in order ***\"To make the derivations mathematically more convenient\"***. You can simply remove the multiplier, see [here](http://mccormickml.com/2014/03/04/gradient-descent-derivation/) for example, and expect the same result.\n",
    "\n",
    "[Where does $\\frac{1}{2m}$ came from ?](https://stackoverflow.com/questions/21099289/cant-understand-the-cost-function-for-linear-regression)\n",
    "\n",
    "```python\n",
    "def costFunctionReg(X,y,theta,lamda = 10):\n",
    "    m = len(y); J = 0;\n",
    "    \n",
    "    h = X @ theta\n",
    "    J = float((1./(2*m)) * (h - y).T @ (h - y)) + (lamda/(2*m)) * np.sum(np.square(theta))\n",
    "    return(J) \n",
    "```\n",
    "[@ PEP 465 -- A dedicated infix operator for matrix multiplication](https://www.python.org/dev/peps/pep-0465/)\n",
    "```python\n",
    "#there are two different ways we might want to define multiplication. \n",
    "#One is elementwise multiplication:\n",
    "[[1, 2],     [[11, 12],     [[1 * 11, 2 * 12],\n",
    " [3, 4]]  x   [13, 14]]  =   [3 * 13, 4 * 14]]\n",
    "#and the other is matrix multiplication\n",
    "[[1, 2],     [[11, 12],     [[1 * 11 + 2 * 13, 1 * 12 + 2 * 14],    [[37, 40],\n",
    " [3, 4]]  x   [13, 14]]  =   [3 * 11 + 4 * 13, 3 * 12 + 4 * 14]]  =  [85, 92]]\n",
    "```\n",
    "The [Python Data Model](https://docs.python.org/3/reference/datamodel.html#emulating-numeric-types) specifies that the ```@``` operator invokes ```__matmul__``` and ```__rmatmul__```.\n",
    "\n",
    "[numpy.dot(a, b, out=None)](https://docs.scipy.org/doc/numpy/reference/generated/numpy.dot.html)\n",
    "Dot product of two arrays. Specifically,\n",
    "* If both ```a``` and ```b``` are **1-D** arrays, it is inner product of vectors (without complex conjugation).\n",
    "* If both ```a``` and ```b``` are **2-D** arrays, it is matrix multiplication, but using matmul or ```a @ b``` is preferred.\n",
    "* If either ```a``` or ```b``` is **0-D** (**scalar**), it is equivalent to multiply and using ```numpy.multiply(a, b)``` or ```a*b``` is preferred.\n",
    "* If ```a``` is an **N-D** array and ```b``` is a **1-D** array, it is a sum product over the last axis of ```a``` and ```b```.\n",
    "* If ```a``` is an **N-D** array and ```b``` is an **M-D** array (where **M>=2**), it is a sum product over the last axis of a and the second-to-last axis of ```b```: \n",
    "```python \n",
    "dot(a, b)[i,j,k,m] = sum(a[i,j,:] * b[k,:,m])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "A = np.matrix('1 2; 3 4')\n",
    "B = np.matrix('11 12; 13 14')\n",
    "C = A @ B; D = np.dot(A,B)\n",
    "print(A); print(B)\n",
    "print(C,C.shape)\n",
    "print(D,D.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gradient**\n",
    "$$\\frac{\\partial J(\\theta)}{\\partial \\theta} = \\frac{1}{m}X^T(X\\theta - y) + \\frac{\\lambda}{m} \\theta$$\n",
    "\n",
    "**Gradient descent (vectorized)**\n",
    "$$\\theta^{(t+1)} : = \\theta^{(t)} - \\alpha \\frac{\\partial}{\\partial \\theta} J(\\theta^{(t)})$$\n",
    "\n",
    "```python\n",
    "def gradient_descent_reg(X,y,theta,alpha = 0.0005,lamda = 10,num_iters=1000):\n",
    "    #Initialisation of useful values \n",
    "    m = np.size(y)\n",
    "    J_history = np.zeros(num_iters)\n",
    "    theta_0_hist, theta_1_hist = [], [] #Used for three D plot\n",
    "\n",
    "    for i in range(num_iters):\n",
    "        #Hypothesis function\n",
    "        h = np.dot(X,theta)\n",
    "        #Grad function in vectorized form\n",
    "        theta = theta - alpha * (1/m)* (  (X.T @ (h-y)) + lamda * theta )\n",
    "        #Cost function in vectorized form       \n",
    "        J_history[i] = costFunctionReg(X,y,theta,lamda)\n",
    "        #Calculate the cost for each iteration(used to plot convergence)\n",
    "        theta_0_hist.append(theta[0,0])\n",
    "        theta_1_hist.append(theta[1,0])   \n",
    "    return theta ,J_history, theta_0_hist, theta_1_hist\n",
    "```\n",
    "\n",
    "**Closed form solution**\n",
    "$$\\boxed{\\theta = (X^TX + \\lambda I)^{-1} X^T Y}$$\n",
    "\n",
    "```python\n",
    "def closed_form_reg_solution(X,y,lamda = 10): \n",
    "    '''Closed form solution for ridge regression'''\n",
    "    m,n = X.shape\n",
    "    I = np.eye((n))\n",
    "    return (np.linalg.inv(X.T@X + lamda*I) @ X.T@y)[:,0]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "#from sklearn import linear_model\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-white')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generating sine curve and uniform noise\n",
    "x = np.linspace(0,1,40)\n",
    "y = np.sin(x * 1.5 * np.pi ) \n",
    "\n",
    "noise = 1*np.random.uniform(  size = 40)\n",
    "y_noise = (y + noise).reshape(-1,1)\n",
    "\n",
    "#Centering the y data\n",
    "y_noise = y_noise - y_noise.mean()\n",
    "\n",
    "#Design matrix is x, x^2\n",
    "X = np.vstack((2*x,x**2)).T\n",
    "\n",
    "#Nornalizing the design matrix to facilitate visualization\n",
    "X = X / np.linalg.norm(X,axis = 0)\n",
    "\n",
    "#Plotting the result\n",
    "plt.scatter(x,y_noise, label = 'Dataset')\n",
    "plt.plot(x,y - y.mean(),label = 'Sine')\n",
    "plt.title('Noisy sine curve')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Setup of meshgrid of theta values\n",
    "T0, T1 = np.meshgrid(np.linspace(7,18,100),np.linspace(-18,-9,100))\n",
    "\n",
    "def costFunctionReg(X,y,theta,lamda = 10):\n",
    "    m = len(y); J = 0;\n",
    "\n",
    "    h = X@theta\n",
    "    J = float((1./(2*m)) *(h - y).T@(h - y)) +(lamda/(2*m)) *np.sum(np.square(theta))\n",
    "    return(J) \n",
    "\n",
    "def gradient_descent_reg(X,y,theta,alpha = 0.0005,lamda = 10,num_iters=1000):\n",
    "    #Initialisation of useful values \n",
    "    m = np.size(y)\n",
    "    J_history = np.zeros(num_iters)\n",
    "    theta_0_hist, theta_1_hist = [], [] #Used for three D plot\n",
    "\n",
    "    for i in range(num_iters):\n",
    "        #Hypothesis function\n",
    "        h = np.dot(X,theta)\n",
    "        #Grad function in vectorized form\n",
    "        theta = theta - alpha * (1/m)* (  (X.T @ (h-y)) + lamda * theta )\n",
    "        #Cost function in vectorized form       \n",
    "        J_history[i] = costFunctionReg(X,y,theta,lamda)\n",
    "        #Calculate the cost for each iteration(used to plot convergence)\n",
    "        theta_0_hist.append(theta[0,0])\n",
    "        theta_1_hist.append(theta[1,0])   \n",
    "    return theta ,J_history, theta_0_hist, theta_1_hist\n",
    "\n",
    "l = 10\n",
    "\n",
    "#Setup of meshgrid of theta values\n",
    "T1, T2 = np.meshgrid(np.linspace(-10,10,100),np.linspace(-10,10,100))\n",
    "\n",
    "#Computing the cost function for each theta combination\n",
    "zs = np.array([costFunctionReg(X, y_noise.reshape(-1,1),np.array([t1,t2]).reshape(-1,1),l) \n",
    "               for t1, t2 in zip(np.ravel(T1), np.ravel(T2)) ]\n",
    "             )\n",
    "#Reshaping the cost values    \n",
    "Z = zs.reshape(T1.shape)\n",
    "\n",
    "\n",
    "#Computing the gradient descent\n",
    "theta_result_reg,J_history_reg, theta_0, theta_1 = gradient_descent_reg(X,y_noise,np.array([7.,10.]).reshape(-1,1), \n",
    "                                                                        0.8,l,num_iters=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from mpl_toolkits import mplot3d\n",
    "\n",
    "#Angles needed for quiver plot\n",
    "anglesx = np.array(theta_0)[1:] - np.array(theta_0)[:-1]\n",
    "anglesy = np.array(theta_1)[1:] - np.array(theta_1)[:-1]\n",
    "\n",
    "%matplotlib inline\n",
    "fig = plt.figure(figsize = (16,8))\n",
    "\n",
    "#Surface plot\n",
    "ax = fig.add_subplot(1, 2, 1, projection='3d')\n",
    "ax.plot_surface(T1, T2, Z, rstride = 5, cstride = 5, cmap = 'jet', alpha=0.5)\n",
    "ax.plot(theta_0,theta_1,J_history_reg, marker = '*', color = 'r', alpha = .4, label = 'Gradient descent')\n",
    "\n",
    "ax.set_xlabel('theta 1')\n",
    "ax.set_ylabel('theta 2')\n",
    "ax.set_zlabel('error')\n",
    "ax.set_title('RSS gradient descent: Root at {}'.format(theta_result_reg.ravel()))\n",
    "ax.view_init(45, -45)\n",
    "\n",
    "\n",
    "#Contour plot\n",
    "ax = fig.add_subplot(1, 2, 2)\n",
    "ax.contour(T1, T2, Z, 100, cmap = 'jet')\n",
    "ax.quiver(theta_0[:-1], theta_1[:-1], anglesx, anglesy, scale_units = 'xy', angles = 'xy', scale = 1, color = 'r', alpha = .9)\n",
    "ax.set_xlabel('theta 1')\n",
    "ax.set_ylabel('theta 2')\n",
    "\n",
    "plt.suptitle('Cost function and gradient descent: Ridge regularization')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [The Simplest Machine Learning Algorithm](https://simplyml.com/the-simplest-machine-learning-algorithm/)\n",
    "[Wine Quality Datasets](http://www3.dsi.uminho.pt/pcortez/wine/)\n",
    "\n",
    "$$min \\sum_{i=1}^n (\\textbf{x}_i^T \\textbf{w} - y_i)^2 + \\lambda \\|\\textbf{w}\\|^2$$\n",
    "\n",
    "$$\\boxed{\\textbf{w} = (\\textbf{X}^T\\textbf{X} + \\lambda \\textbf{I})^{-1}\\textbf{X}^T\\textbf{y} }$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#The Simplest Machine Learning Algorithm\n",
    "#https://simplyml.com/the-simplest-machine-learning-algorithm/\n",
    "import numpy # as np\n",
    "import sklearn\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "#from sklearn.cross_validation import train_test_split\n",
    "#import matplotlib.pyplot as plt\n",
    "#from IPython.display import display, Math, Latex\n",
    "\n",
    "class RidgeRegression(object):\n",
    "    def __init__(self, lmbda=0.1):\n",
    "        self.lmbda = lmbda\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        C = X.T.dot(X) + self.lmbda*numpy.eye(X.shape[1])\n",
    "        self.w = numpy.linalg.inv(C).dot(X.T.dot(y))\n",
    "\n",
    "    def predict(self, X):\n",
    "        return X.dot(self.w)\n",
    "\n",
    "    def get_params(self, deep=True):\n",
    "        return {\"lmbda\": self.lmbda}\n",
    "\n",
    "    def set_params(self, lmbda=0.1):\n",
    "        self.lmbda = lmbda\n",
    "        return self\n",
    "\n",
    "Xy = numpy.loadtxt(\"winequality/winequality-white.csv\", delimiter=\";\", skiprows=1)\n",
    "\n",
    "#X = Xy[:, 0:-1]\n",
    "X = Xy[:, :-1]\n",
    "X = preprocessing.scale(X)\n",
    "\n",
    "y = Xy[:, -1]\n",
    "y -= y.mean()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "ridge = RidgeRegression()\n",
    "param_grid = [{\"lmbda\": 2.0**numpy.arange(-5, 10)}]\n",
    "#learner = sklearn.model_selection.GridSearchCV(ridge, param_grid, scoring=\"mean_absolute_error\", n_jobs=-1, verbose=0)\n",
    "#sorted(sklearn.metrics.SCORERS.keys())\n",
    "learner = sklearn.model_selection.GridSearchCV(ridge, param_grid, scoring=\"neg_mean_absolute_error\", n_jobs=-1, verbose=0)\n",
    "learner.fit(X_train, y_train)\n",
    "\n",
    "y_pred = learner.predict(X_test)\n",
    "ridge_error = sklearn.metrics.mean_absolute_error(y_test, y_pred)\n",
    "print(ridge_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Переопределённая система](https://ru.wikipedia.org/wiki/%D0%9F%D0%B5%D1%80%D0%B5%D0%BE%D0%BF%D1%80%D0%B5%D0%B4%D0%B5%D0%BB%D1%91%D0%BD%D0%BD%D0%B0%D1%8F_%D1%81%D0%B8%D1%81%D1%82%D0%B5%D0%BC%D0%B0)\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "<td style=\"width:60%; vertical-align:top; text-align:left; font-size:medium;\">\n",
    "<b>Переопределённая система</b> — система, число уравнений которой больше числа неизвестных.\n",
    "\n",
    "Для однозначного решения линейной системы уравнений нужно иметь $n$ уравнений при $n$ переменных величинах. Если уравнений меньше, чем число переменных величин, то такая система $\\textit{не определена}$ (или $\\textit{несовместна}$, см. следствие 2 в [Метод Гаусса](https://ru.wikipedia.org/wiki/%D0%9C%D0%B5%D1%82%D0%BE%D0%B4_%D0%93%D0%B0%D1%83%D1%81%D1%81%D0%B0)). Также система $n$ (или больше) уравнений может быть $\\textit{недоопределена}$, если некоторые уравнения не поставляют никакую дополнительную независимую от других уравнений информацию.\n",
    "\n",
    "В силу отсутствия точного решения переопределённых систем, на практике принято вместо него отыскивать вектор, наилучшим образом удовлетворяющий всем уравнениям, то есть минимизирующий норму [невязки](https://ru.wikipedia.org/wiki/%D0%9D%D0%B5%D0%B2%D1%8F%D0%B7%D0%BA%D0%B0) системы в какой-нибудь степени. Этой проблеме посвящён отдельный раздел [математической статистики](https://ru.wikipedia.org/wiki/%D0%9C%D0%B0%D1%82%D0%B5%D0%BC%D0%B0%D1%82%D0%B8%D1%87%D0%B5%D1%81%D0%BA%D0%B0%D1%8F_%D1%81%D1%82%D0%B0%D1%82%D0%B8%D1%81%D1%82%D0%B8%D0%BA%D0%B0) — [регрессионный анализ](https://ru.wikipedia.org/wiki/%D0%A0%D0%B5%D0%B3%D1%80%D0%B5%D1%81%D1%81%D0%B8%D0%BE%D0%BD%D0%BD%D1%8B%D0%B9_%D0%B0%D0%BD%D0%B0%D0%BB%D0%B8%D0%B7). Наиболее часто минимизируют квадрат отклонений от оцениваемого решения. Для этого применяют так называемый [метод наименьших квадратов](https://ru.wikipedia.org/wiki/%D0%9C%D0%B5%D1%82%D0%BE%D0%B4_%D0%BD%D0%B0%D0%B8%D0%BC%D0%B5%D0%BD%D1%8C%D1%88%D0%B8%D1%85_%D0%BA%D0%B2%D0%B0%D0%B4%D1%80%D0%B0%D1%82%D0%BE%D0%B2).\n",
    "</td>\n",
    "<td>\n",
    "<img src=\"images/3_equations_-1.jpg\" />\n",
    "</td>\n",
    "<td style=\"width:15%;\">\n",
    "    The equations<br>\n",
    "    $x − 2y = −1$,<br> \n",
    "    $3x + 5y = 8$,<br> \n",
    "    $4x + 3y = 7$<br> are linearly dependent.\n",
    "<img src=\"images/600px-Three_Intersecting_Lines.svg.png\" />\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "# [Невязка](https://ru.wikipedia.org/wiki/%D0%9D%D0%B5%D0%B2%D1%8F%D0%B7%D0%BA%D0%B0)\n",
    "\n",
    "**Невязка** — величина ошибки (расхождения) приближённого равенства.\n",
    "\n",
    "<table align=\"left\" style=\"font-size:medium;\">\n",
    "<tr>\n",
    "<td style=\"white-space:nowrap; vertical-align:top; text-align:left; font-size:medium;\">\n",
    "Пусть требуется найти такое $x$, что значение функции: \n",
    "</td>\n",
    "<td style=\"vertical-align:top; text-align:left; font-size:medium;\">\n",
    "${\\displaystyle f(x)=b}$\n",
    "</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td style=\"width:10%; white-space:nowrap; vertical-align:top; text-align:left; font-size:medium;\">\n",
    "Подставив приближенное значение $x_0$ вместо $x$, получаем невязку\n",
    "</td>\n",
    "<td style=\"vertical-align:top; text-align:left; font-size:medium;\">\n",
    "${\\displaystyle b-f(x_{0})}$\n",
    "</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td style=\"white-space:nowrap; vertical-align:top; font-size:medium; text-align:center;\">\n",
    "а ошибка в этом случае равна \n",
    "</td>\n",
    "<td style=\"vertical-align:top; text-align:left; font-size:medium;\">\n",
    "${\\displaystyle x_{0}-x}$\n",
    "</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td colspan=\"2\" style=\"vertical-align:top; font-size:medium; text-align:left;\">\n",
    "Если точное значение $x$ неизвестно, вычисление ошибки невозможно, однако при этом может быть определена невязка.\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "# [несовместные системы линейных уравнений](https://en.wikipedia.org/wiki/Overdetermined_system)\n",
    "\n",
    "    \n",
    "$\\begin{cases}\n",
    "Y = 2X -1\\\\\n",
    "Y = 3X -2\\\\\n",
    "Y = X +1\n",
    "\\end{cases}\n",
    " = \\text{может быть записана в матричной форме} = \n",
    "\\begin{bmatrix}\n",
    "2 & 1\\\\\n",
    "-3 & 1\\\\\n",
    "-1 & 1\\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "X\\\\\n",
    "Y\\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "-1\\\\\n",
    "-2\\\\\n",
    "1\\\\\n",
    "\\end{bmatrix}\n",
    "$\n",
    "<table>\n",
    "<tr>\n",
    "<td style=\"width:25%; vertical-align:top;\">\n",
    "#1 A system of three linearly independent equations, three lines, no solutions\n",
    "<img src=\"images/3_equations_-1.jpg\" />\n",
    "</td>\n",
    "<td>\n",
    "#2 A system of three linearly independent equations, three lines (two parallel), no solutions\n",
    "<img src=\"images/3_equations_-2.jpg\" />\n",
    "</td>\n",
    "<td style=\"vertical-align:top;\">\n",
    "#3 A system of three linearly independent equations, three lines (all parallel), no solutions\n",
    "<img src=\"images/3_equations_-3.jpg\" />\n",
    "</td>\n",
    "<td>\n",
    "#4  A system of three equations (one equation linearly dependent on the others), three lines (two coinciding), one solution\n",
    "<img src=\"images/3_equations_-4.jpg\" />\n",
    "</td>\n",
    "<td>\n",
    "#5  A system of three equations (one equation linearly dependent on the others), three lines (two coinciding), one solution\n",
    "<img src=\"images/3_equations_-5.jpg\" />\n",
    "</td>\n",
    "<td>\n",
    "#6  A system of three equations (one equation linearly dependent on the others), three lines (two coinciding), one solution\n",
    "<img src=\"images/3_equations_-6.jpg\" />\n",
    "</td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ridge and Lasso: Geometric Interpretation\n",
    "#https://www.astroml.org/book_figures/chapter8/fig_lasso_ridge.html\n",
    "\n",
    "# Author: Jake VanderPlas\n",
    "# License: BSD\n",
    "#   The figure produced by this code is published in the textbook\n",
    "#   \"Statistics, Data Mining, and Machine Learning in Astronomy\" (2013)\n",
    "#   For more information, see http://astroML.github.com\n",
    "#   To report a bug or issue, use the following forum:\n",
    "#    https://groups.google.com/forum/#!forum/astroml-general\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.patches import Ellipse, Circle, RegularPolygon\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "# This function adjusts matplotlib settings for a uniform feel in the textbook.\n",
    "# Note that with usetex=True, fonts are rendered with LaTeX.  This may\n",
    "# result in an error if LaTeX is not installed on your system.  In that case,\n",
    "# you can set usetex to False.\n",
    "#if \"setup_text_plots\" not in globals():\n",
    "#    from astroML.plotting import setup_text_plots\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Set up figure\n",
    "fig = plt.figure(figsize=(5, 2.5), facecolor='w')\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# plot ridge diagram\n",
    "ax = fig.add_axes([0, 0, 0.5, 1], frameon=False, xticks=[], yticks=[])\n",
    "\n",
    "# plot the axes\n",
    "ax.arrow(-1, 0, 9, 0, head_width=0.1, fc='k')\n",
    "ax.arrow(0, -1, 0, 9, head_width=0.1, fc='k')\n",
    "\n",
    "# plot the ellipses and circles\n",
    "for i in range(3):\n",
    "    ax.add_patch(Ellipse((3, 5),\n",
    "                         3.5 * np.sqrt(2 * i + 1), 1.7 * np.sqrt(2 * i + 1),\n",
    "                         -15, fc='none'))\n",
    "\n",
    "ax.add_patch(Circle((0, 0), 3.815, fc='none'))\n",
    "\n",
    "# plot arrows\n",
    "ax.arrow(0, 0, 1.46, 3.52, head_width=0.2, fc='k',\n",
    "         length_includes_head=True)\n",
    "ax.arrow(0, 0, 3, 5, head_width=0.2, fc='k',\n",
    "         length_includes_head=True)\n",
    "ax.arrow(0, -0.2, 3.81, 0, head_width=0.1, fc='k',\n",
    "         length_includes_head=True)\n",
    "ax.arrow(3.81, -0.2, -3.81, 0, head_width=0.1, fc='k',\n",
    "         length_includes_head=True)\n",
    "\n",
    "# annotate with text\n",
    "ax.text(7.5, -0.1, r'$\\theta_1$', va='top')\n",
    "ax.text(-0.1, 7.5, r'$\\theta_2$', ha='right')\n",
    "ax.text(3, 5 + 0.2, r'$\\rm \\theta_{normal\\ equation}$',\n",
    "        ha='center', bbox=dict(boxstyle='round', ec='k', fc='w'))\n",
    "ax.text(1.46, 3.52 + 0.2, r'$\\rm \\theta_{ridge}$', ha='center',\n",
    "        bbox=dict(boxstyle='round', ec='k', fc='w'))\n",
    "ax.text(1.9, -0.3, r'$r$', ha='center', va='top')\n",
    "\n",
    "ax.set_xlim(-2, 9)\n",
    "ax.set_ylim(-2, 9)\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# plot lasso diagram\n",
    "ax = fig.add_axes([0.5, 0, 0.5, 1], frameon=False, xticks=[], yticks=[])\n",
    "\n",
    "# plot axes\n",
    "ax.arrow(-1, 0, 9, 0, head_width=0.1, fc='k')\n",
    "ax.arrow(0, -1, 0, 9, head_width=0.1, fc='k')\n",
    "\n",
    "# plot ellipses and circles\n",
    "for i in range(3):\n",
    "    ax.add_patch(Ellipse((3, 5),\n",
    "                         3.5 * np.sqrt(2 * i + 1), 1.7 * np.sqrt(2 * i + 1),\n",
    "                         -15, fc='none'))\n",
    "\n",
    "# this is producing some weird results on save\n",
    "#ax.add_patch(RegularPolygon((0, 0), 4, 4.4, np.pi, fc='none'))\n",
    "ax.plot([-4.4, 0, 4.4, 0, -4.4], [0, 4.4, 0, -4.4, 0], '-k')\n",
    "\n",
    "# plot arrows\n",
    "ax.arrow(0, 0, 0, 4.4, head_width=0.2, fc='k', length_includes_head=True)\n",
    "ax.arrow(0, 0, 3, 5, head_width=0.2, fc='k', length_includes_head=True)\n",
    "ax.arrow(0, -0.2, 4.2, 0, head_width=0.1, fc='k', length_includes_head=True)\n",
    "ax.arrow(4.2, -0.2, -4.2, 0, head_width=0.1, fc='k', length_includes_head=True)\n",
    "\n",
    "# annotate plot\n",
    "ax.text(7.5, -0.1, r'$\\theta_1$', va='top')\n",
    "ax.text(-0.1, 7.5, r'$\\theta_2$', ha='right')\n",
    "ax.text(3, 5 + 0.2, r'$\\rm \\theta_{normal\\ equation}$',\n",
    "        ha='center', bbox=dict(boxstyle='round', ec='k', fc='w'))\n",
    "ax.text(0, 4.4 + 0.2, r'$\\rm \\theta_{lasso}$', ha='center',\n",
    "        bbox=dict(boxstyle='round', ec='k', fc='w'))\n",
    "ax.text(2, -0.3, r'$r$', ha='center', va='top')\n",
    "\n",
    "ax.set_xlim(-2, 9)\n",
    "ax.set_ylim(-2, 9)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
